
import asyncio
import json
import os
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
import openai

class NLToSchemaClient:
    def __init__(self, api_key: str, base_url: str = None, model: str = "gpt-4"):
        self.llm = openai.AsyncOpenAI(api_key=api_key, base_url=base_url)
        self.model = model
        
    async def run(self, server_command: list[str]):
        """Main chat loop"""
        server_params = StdioServerParameters(
            command=server_command[0], 
            args=server_command[1:]
        )
        
        async with stdio_client(server_params) as (read, write):
            async with ClientSession(read, write) as session:
                await session.initialize()
                
                # Get capabilities
                tools = await session.list_tools()
                openai_tools = [self._to_openai_tool(t) for t in tools.tools]
                
                print("Ready! Commands: 'list prompts', 'get prompt <name>', or enter requests. Type 'quit' to exit:")
                
                while True:
                    user_input = input("\n> ").strip()
                    if user_input.lower() in ['quit', 'exit']:
                        break
                    if not user_input:
                        continue
                        
                    await self._handle_request(session, openai_tools, user_input)
    
    async def _handle_request(self, session, tools, user_input):
        """Handle single user request with prompt support"""
        # List prompts
        if user_input.lower() in ['list prompts', 'show prompts']:
            await self._list_prompts(session)
            return
        
        # Get specific prompt
        if user_input.lower().startswith('get prompt '):
            prompt_name = user_input[11:].strip()
            await self._get_prompt(session, prompt_name)
            return
        
        # Regular LLM conversation
        messages = [{"role": "user", "content": user_input}]
        
        for _ in range(3):  # Max 3 iterations
            response = await self.llm.chat.completions.create(
                model=self.model,
                messages=messages,
                tools=tools,
                tool_choice="auto"
            )
            
            message = response.choices[0].message
            
            if not message.tool_calls:
                print(f"\n{message.content}")
                break
                
            messages.append({
                "role": "assistant", 
                "content": message.content,
                "tool_calls": [{"id": tc.id, "type": "function", 
                              "function": {"name": tc.function.name, 
                                         "arguments": tc.function.arguments}} 
                             for tc in message.tool_calls]
            })
            
            for tool_call in message.tool_calls:
                try:
                    args = json.loads(tool_call.function.arguments)
                    result = await session.call_tool(tool_call.function.name, args)
                    content = result.content[0].text if result.content else "Done"
                except Exception as e:
                    content = f"Error: {e}"
                    
                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "content": content
                })
    
    async def _list_prompts(self, session):
        """List available prompts"""
        try:
            result = await session.list_prompts()
            if result.prompts:
                print("\nAvailable prompts:")
                for prompt in result.prompts:
                    args_info = f" (args: {', '.join(prompt.arguments)})" if hasattr(prompt, 'arguments') and prompt.arguments else ""
                    print(f"- {prompt.name}: {prompt.description}{args_info}")
            else:
                print("No prompts available")
        except Exception as e:
            print(f"Error listing prompts: {e}")
    
    async def _get_prompt(self, session, name: str, arguments: dict = None):
        """Get and display a specific prompt"""
        try:
            result = await session.get_prompt(name, arguments or {})
            print(f"\nPrompt '{name}':")
            for msg in result.messages:
                role = msg.role.upper()
                content = msg.content
                print(f"\n{role}:\n{content}")
        except Exception as e:
            print(f"Error getting prompt '{name}': {e}")
    
    def _to_openai_tool(self, mcp_tool):
        """Convert MCP tool to OpenAI format"""
        return {
            "type": "function",
            "function": {
                "name": mcp_tool.name,
                "description": mcp_tool.description,
                "parameters": mcp_tool.inputSchema
            }
        }

async def main():
    api_key = os.getenv("OPENAI_API_KEY")
    base_url = os.getenv("OPENAI_BASE_URL")
    
    if not api_key:
        print("Set OPENAI_API_KEY environment variable")
        return
        
    client = NLToSchemaClient(api_key, base_url)
    server_cmd = os.getenv("MCP_SERVER_CMD", "python your_formatter_server.py").split()
    await client.run(server_cmd)

if __name__ == "__main__":
    asyncio.run(main())