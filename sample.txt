Perfect. Below is a skeleton FastAPI app for your Azure OpenAI RAG + Function Calling pipeline, clean and extensible for your hackathon use.

‚∏ª

üóÇÔ∏è Directory Structure

rag_app/
‚îú‚îÄ‚îÄ main.py                # FastAPI entry point
‚îú‚îÄ‚îÄ llm_router.py          # Function calling classification (Azure OpenAI)
‚îú‚îÄ‚îÄ retriever.py           # Handles both code/docs vector DB retrieval
‚îú‚îÄ‚îÄ context_builder.py     # Enhances chunks with metadata
‚îú‚îÄ‚îÄ generator.py           # LLM response generator
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ config.py              # Azure & vector DB config


‚∏ª

üìÑ main.py

from fastapi import FastAPI, Request
from pydantic import BaseModel

from llm_router import route_query
from retriever import retrieve_context
from context_builder import build_context
from generator.py import generate_answer

app = FastAPI()

class Query(BaseModel):
    question: str

@app.post("/ask")
async def ask_question(payload: Query):
    question = payload.question

    route = await route_query(question)
    chunks = await retrieve_context(question, route)
    context = build_context(chunks)
    answer = await generate_answer(question, context)

    return {"answer": answer, "context": context}


‚∏ª

üìÑ llm_router.py

import openai
from config import AZURE_DEPLOYMENT_NAME

# Function schema
FUNCTIONS = [{
    "name": "route_query",
    "description": "Classifies the user query to determine which database(s) to query and how.",
    "parameters": {
        "type": "object",
        "properties": {
            "target_db": {
                "type": "string",
                "enum": ["code", "docs", "both"]
            },
            "strategy": {
                "type": "string",
                "enum": ["semantic", "structured"]
            }
        },
        "required": ["target_db", "strategy"]
    }
}]

async def route_query(question: str):
    response = openai.ChatCompletion.create(
        engine=AZURE_DEPLOYMENT_NAME,
        messages=[{"role": "user", "content": question}],
        functions=FUNCTIONS,
        function_call="auto"
    )

    function_args = response.choices[0].message.get("function_call", {}).get("arguments", "{}")
    return eval(function_args)  # quick hack: parse JSON dict string


‚∏ª

üìÑ retriever.py

# Assume code_vector_db and docs_vector_db are initialized clients
from config import code_vector_db, docs_vector_db

async def retrieve_context(question: str, route: dict):
    strategy = route["strategy"]
    db = route["target_db"]

    results = []

    if db in ["docs", "both"]:
        results += docs_vector_db.search(question, strategy)

    if db in ["code", "both"]:
        results += code_vector_db.search(question, strategy)

    return results


‚∏ª

üìÑ context_builder.py

def build_context(chunks):
    return [
        f"[{c['metadata'].get('source', '')} > {c['metadata'].get('section', '')}]\n{c['content']}"
        for c in chunks
    ]


‚∏ª

üìÑ generator.py

import openai
from config import AZURE_DEPLOYMENT_NAME

SYSTEM_PROMPT = "Use the retrieved context to answer the user's question accurately. If unsure, say so."

async def generate_answer(question: str, context_chunks: list):
    full_context = "\n\n".join(context_chunks)
    
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": f"Question: {question}\n\nContext:\n{full_context}"}
    ]

    response = openai.ChatCompletion.create(
        engine=AZURE_DEPLOYMENT_NAME,
        messages=messages
    )

    return response.choices[0].message["content"]


‚∏ª

üìÑ config.py

import openai

# Azure OpenAI setup
openai.api_type = "azure"
openai.api_base = "https://YOUR_RESOURCE_NAME.openai.azure.com/"
openai.api_version = "2023-07-01-preview"
openai.api_key = "YOUR_API_KEY"
AZURE_DEPLOYMENT_NAME = "gpt-4"

# Mock vector DBs (replace with your actual vector search clients)
class MockVectorDB:
    def search(self, query, strategy):
        return [{"content": f"Dummy content for {query}", "metadata": {"source": "example", "section": "intro"}}]

code_vector_db = MockVectorDB()
docs_vector_db = MockVectorDB()


‚∏ª

üìÑ requirements.txt

fastapi
openai
uvicorn
pydantic


‚∏ª

üèÅ Run the App

uvicorn main:app --reload


‚∏ª

Let me know if you want:
	‚Ä¢	Auth/caching/logging added
	‚Ä¢	LangChain integration
	‚Ä¢	Async vector DB calls (e.g., Qdrant, Pinecone)

Otherwise, you‚Äôre good to build and iterate for your hackathon.